{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Task 1 Clean and Upload Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "import csv\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connecting to GBQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "client = bigquery.Client(project = \"wedge_project_np\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Goggle Big Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_config = bigquery.LoadJobConfig()\n",
    "job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
    "job_config.schema_update_options = [bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION]\n",
    "\n",
    "#job_config.skip_leading_rows = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opening Zip Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(\"data/SmallZip\"):\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterate Over Files and Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"data/SmallZip/\"\n",
    "zip_files = os.listdir(\"data/SmallZip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_exists(client, table_ref):\n",
    "    try:\n",
    "        client.get_table(table_ref)\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in zip_files :\n",
    "    with ZipFile(data_directory + file,'r') as zf :  \n",
    "        print(zf.namelist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### How is this different compared to below. I didn't extract below and need to??\n",
    "\n",
    "#with ZipFile(data_directory+file) as my_zip:\n",
    "    #print(my_zip.namelist())\n",
    "    #for zipped_file in my_zip.namelist():\n",
    "        #x = my_zip.extract(zipped_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#current_zf = zip_files[0]\n",
    "for current_zf in zip_files :\n",
    "# Open the current zipfile\n",
    "    with ZipFile(data_directory + current_zf,'r') as zf :\n",
    "        zipped_files = zf.namelist()\n",
    "        \n",
    "        # Iteraate over each file inside the current zip file\n",
    "        for file_name in zipped_files :\n",
    "            # Open and wrap it to read as text\n",
    "            input_file = io.TextIOWrapper(zf.open(file_name, 'r'), encoding=\"utf-8\")\n",
    "\n",
    "            for idx, line in enumerate(input_file) :\n",
    "                print(line)\n",
    "                if idx > 4 :\n",
    "                    break\n",
    "            input_file.close()\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sniffing out the Delimiter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiters = dict() \n",
    "\n",
    "# Start by reading in all the files again.\n",
    "\n",
    "#current_zf = zip_files[0]\n",
    "for current_zf in zip_files :\n",
    "# Open the current zf\n",
    "    with ZipFile(data_directory + current_zf,'r') as zf :\n",
    "        zipped_files = zf.namelist()\n",
    "        \n",
    "        # Iteraate over each file inside the current zip file\n",
    "        for file_name in zipped_files :\n",
    "            # Open and wrap it to read as text\n",
    "            input_file = io.TextIOWrapper(zf.open(file_name, 'r'), encoding=\"utf-8\")\n",
    "            \n",
    "            dialect = csv.Sniffer().sniff(sample=input_file.readline(),\n",
    "                                      delimiters=[\",\",\";\",\"\\t\"])\n",
    "            \n",
    "            delimiters[file_name] = dialect.delimiter\n",
    "            \n",
    "            print(\" \".join([\"For\",\n",
    "                           file_name,\n",
    "                           \"the delimiter is\",\n",
    "                           dialect.delimiter\n",
    "                           ]))\n",
    "\n",
    "            input_file.close() # tidy up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for Headers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to look for headers\n",
    "\n",
    "def is_header_row(first_row, second_row):\n",
    "    \"\"\"\n",
    "    Function to determine if the first row is a header based on the types of data in the rows.\n",
    "    Returns True if the first row looks like a header.\n",
    "    \"\"\"\n",
    "    # Check if the first row contains non-numeric values and second row contains numeric values\n",
    "    for first, second in zip(first_row, second_row):\n",
    "        # If the first value is not a number but the second value is, it's likely a header\n",
    "        if first.isdigit() == False and second.isdigit() == True:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = dict()\n",
    "delimiters = dict() \n",
    "\n",
    "for current_zf in zip_files :\n",
    "    with ZipFile(data_directory + current_zf,'r') as zf :\n",
    "        zipped_files = zf.namelist()\n",
    "\n",
    "        for file_name in zipped_files :\n",
    "            # Open and wrap it to read as text\n",
    "            input_file = io.TextIOWrapper(zf.open(file_name, 'r'), encoding=\"utf-8\")\n",
    "            \n",
    "            dialect = csv.Sniffer().sniff(sample=input_file.readline(),\n",
    "                                      delimiters=[\",\",\";\",\"\\t\"])\n",
    "            \n",
    "            delimiters[file_name] = dialect.delimiter\n",
    "            \n",
    "            print(\" \".join([\"For\",\n",
    "                           file_name,\n",
    "                           \"the delimiter is\",\n",
    "                           dialect.delimiter\n",
    "                           ]))\n",
    "            \n",
    "            this_delimiter = delimiters[file_name]\n",
    "\n",
    "            # Read the first two lines of the file\n",
    "            first_line = input_file.readline().strip().split(this_delimiter)\n",
    "            second_line = input_file.readline().strip().split(this_delimiter)\n",
    "            \n",
    "            # Check if the first line is a header row\n",
    "            has_header = is_header_row(first_line, second_line)\n",
    "            \n",
    "            # Print if it has a header\n",
    "            if has_header:\n",
    "                print(f\"File {file_name} has a header row.\")\n",
    "            else:\n",
    "                print(f\"File {file_name} does NOT have a header row.\")\n",
    "            \n",
    "            # Append result to headers dictionary\n",
    "            headers[file_name] = has_header\n",
    "\n",
    "            input_file.close() # tidy up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read through Zip files, find delimeters, find headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = dict()\n",
    "delimiters = dict() \n",
    "\n",
    "for current_zf in zip_files :\n",
    "    with ZipFile(data_directory + current_zf,'r') as zf :\n",
    "        zipped_files = zf.namelist()\n",
    "\n",
    "        for file_name in zipped_files :\n",
    "            # Open and wrap it to read as text\n",
    "            input_file = io.TextIOWrapper(zf.open(file_name, 'r'), encoding=\"utf-8\")\n",
    "            \n",
    "            dialect = csv.Sniffer().sniff(sample=input_file.readline(),\n",
    "                                      delimiters=[\",\",\";\",\"\\t\"])\n",
    "            \n",
    "            delimiters[file_name] = dialect.delimiter\n",
    "            \n",
    "            print(\" \".join([\"For\",\n",
    "                           file_name,\n",
    "                           \"the delimiter is\",\n",
    "                           dialect.delimiter\n",
    "                           ]))\n",
    "            \n",
    "            this_delimiter = delimiters[file_name]\n",
    "\n",
    "            # Read the first two lines of the file\n",
    "            first_line = input_file.readline().strip().split(this_delimiter)\n",
    "            second_line = input_file.readline().strip().split(this_delimiter)\n",
    "            \n",
    "            # Check if the first line is a header row\n",
    "            has_header = is_header_row(first_line, second_line)\n",
    "            \n",
    "            # Print if it has a header\n",
    "            if has_header:\n",
    "                print(f\"File {file_name} has a header row.\")\n",
    "                header_option = 0  # The first line is a header\n",
    "            else:\n",
    "                print(f\"File {file_name} does NOT have a header row.\")\n",
    "                header_option = None  # No header present\n",
    "\n",
    "            # Append result to headers dictionary\n",
    "            headers[file_name] = has_header\n",
    "\n",
    "            #Read into a dataframe\n",
    "            try:\n",
    "                # Read into a DataFrame, assuming the first line might be a header\n",
    "                df = pd.read_csv(input_file, delimiter=this_delimiter, header = 0)\n",
    "\n",
    "                # Print the first few rows of the DataFrame to confirm\n",
    "                print(f\"DataFrame from {file_name}:\")\n",
    "                print(df.head())  # Show the first 5 rows of the DataFrame\n",
    "\n",
    "            except Exception as e:\n",
    "                # Handle the case where reading fails\n",
    "                print(f\"Error reading {file_name}: {e}\")\n",
    "\n",
    "            input_file.close() # tidy up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_config.schema = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_path= data_directory + current_zf\n",
    "if os.path.exists(full_path):\n",
    "    print(\"File found\")\n",
    "\n",
    "    with open(data_directory + current_zf, 'rb') as source_file:\n",
    "        job = client.load_table_from_file(\n",
    "            source_file,\n",
    "            f\"{gbq_dataset_id}.{current_zf}\",\n",
    "            job_config=job_config,\n",
    "        )\n",
    "else:\n",
    "    print(\"File not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create table names for tables in GBQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for file in data_directory:\n",
    "    tab, _ = file.replace(\"_clean.csv\", \"\")\n",
    "    table_full_name = \".\".join([gbq_project_id, gbq_dataset_id, tab])\n",
    "\n",
    "    if not table_exists(client, table_full_name):\n",
    "        table_ref = client.create_table(table=table_full_name)\n",
    "    else:\n",
    "        table_ref = client.get_table(table_full_name)\n",
    "\n",
    "    table = client.get_table(table_ref)\n",
    "    print(\"Table {} contains {} columns\".format(table_ref.table_id, len(table.schema)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab, _ = file.replace(\"_clean.csv\", \"\")\n",
    "table_full_name = \".\".join([gbq_project_id, gbq_dataset_id, tab])\n",
    "\n",
    "if not table_exists(client, table_full_name):\n",
    "    table_ref = client.create_table(table=table_full_name)\n",
    "else:\n",
    "    table_ref = client.get_table(table_full_name)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
