{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pandas_gbq import to_gbq\n",
    "from google.cloud import bigquery\n",
    "\n",
    "\n",
    "import re\n",
    "##import datetime \n",
    "\n",
    "\n",
    "#import numpy as np\n",
    "#import pandas_gbq\n",
    "#import janitor\n",
    "\n",
    "#from google.oauth2 import service_account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Goggle Big Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbq_project_ID = \"wedge-project-np\"\n",
    "gbq_dataset_ID = \"wedge\"\n",
    "\n",
    "client = bigquery.Client(project = gbq_project_ID)\n",
    "credentials = client._credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Data Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"Data/processed_files/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Big Query schema\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"datetime\", \"TIMESTAMP\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"register_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"emp_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"upc\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"description\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_type\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_subtype\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_status\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"department\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"quantity\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"Scale\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"cost\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"unitPrice\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"total\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"regPrice\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"altPrice\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"tax\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"taxexempt\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"foodstamp\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"wicable\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"discount\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"memDiscount\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"discountable\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"discounttype\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"voided\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"percentDiscount\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"ItemQtty\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"volDiscType\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"volume\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"VolSpecial\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"mixMatch\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"matched\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"memType\", \"BOOLEAN\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"staff\", \"BOOLEAN\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"numflag\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"itemstatus\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"tenderstatus\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"charflag\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"varflag\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"batchHeaderID\", \"BOOLEAN\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"local\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"organic\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"display\", \"BOOLEAN\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"receipt\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"card_no\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"store\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"branch\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"match_ID\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"trans_ID\", \"FLOAT\", mode=\"NULLABLE\")\n",
    "]\n",
    "\n",
    "# Set the schema in the job config\n",
    "job_config = bigquery.LoadJobConfig(schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Big Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#job_config = bigquery.LoadJobConfig()\n",
    "#job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n",
    "#job_config.schema_update_options = [bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION]\n",
    "#skip_leading_rows=1,  # Skip header row\n",
    "\n",
    "#job_config.skip_leading_rows = 1\n",
    "\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    schema=schema,\n",
    "    source_format=bigquery.SourceFormat.CSV,\n",
    "    skip_leading_rows=1,  # Skip header row\n",
    "    write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
    "    schema_update_options = [bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION]  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_files = os.listdir(data_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 997.22it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 998.88it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "C:\\Users\\npleg\\AppData\\Local\\Temp\\ipykernel_15628\\3771781157.py:2: DtypeWarning: Columns (42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df= pd.read_csv(data_directory + file_name)\n",
      "Error converting Pandas column with name: \"organic\" and datatype: \"object\" to an appropriate pyarrow datatype: Array, ListArray, or StructArray\n"
     ]
    },
    {
     "ename": "ArrowTypeError",
     "evalue": "Error converting Pandas column with name: \"organic\" and datatype: \"object\" to an appropriate pyarrow datatype: Array, ListArray, or StructArray",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowTypeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\npleg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\cloud\\bigquery\\_pandas_helpers.py:340\u001b[0m, in \u001b[0;36mbq_to_arrow_array\u001b[1;34m(series, bq_field)\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pyarrow\u001b[38;5;241m.\u001b[39mStructArray\u001b[38;5;241m.\u001b[39mfrom_pandas(series, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39marrow_type)\n\u001b[1;32m--> 340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpyarrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mArray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marrow_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pyarrow\u001b[38;5;241m.\u001b[39mArrowTypeError:\n",
      "File \u001b[1;32mc:\\Users\\npleg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyarrow\\array.pxi:1124\u001b[0m, in \u001b[0;36mpyarrow.lib.Array.from_pandas\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\npleg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyarrow\\array.pxi:358\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\npleg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyarrow\\array.pxi:85\u001b[0m, in \u001b[0;36mpyarrow.lib._ndarray_to_array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\npleg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowTypeError\u001b[0m: Expected bytes, got a 'float' object",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mArrowTypeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m table_name \u001b[38;5;241m=\u001b[39m file_name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_processed.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m full_table_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgbq_dataset_ID\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mto_gbq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_table_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgbq_project_ID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreplace\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\npleg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas_gbq\\gbq.py:1181\u001b[0m, in \u001b[0;36mto_gbq\u001b[1;34m(dataframe, destination_table, project_id, chunksize, reauth, if_exists, auth_local_webserver, table_schema, location, progress_bar, credentials, api_method, verbose, private_key, auth_redirect_uri, client_id, client_secret)\u001b[0m\n\u001b[0;32m   1176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataframe\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m   1177\u001b[0m     \u001b[38;5;66;03m# Create the table (if needed), but don't try to run a load job with an\u001b[39;00m\n\u001b[0;32m   1178\u001b[0m     \u001b[38;5;66;03m# empty file. See: https://github.com/pydata/pandas-gbq/issues/237\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m-> 1181\u001b[0m \u001b[43mconnector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdestination_table_ref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_disposition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_disposition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtable_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbilling_project\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1190\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\npleg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas_gbq\\gbq.py:545\u001b[0m, in \u001b[0;36mGbqConnector.load_data\u001b[1;34m(self, dataframe, destination_table_ref, write_disposition, chunksize, schema, progress_bar, api_method, billing_project)\u001b[0m\n\u001b[0;32m    542\u001b[0m total_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataframe)\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 545\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_chunks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_table_ref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrite_disposition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_disposition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbilling_project\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbilling_project\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m progress_bar \u001b[38;5;129;01mand\u001b[39;00m tqdm:\n\u001b[0;32m    557\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m tqdm\u001b[38;5;241m.\u001b[39mtqdm(chunks)\n",
      "File \u001b[1;32mc:\\Users\\npleg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas_gbq\\load.py:251\u001b[0m, in \u001b[0;36mload_chunks\u001b[1;34m(client, dataframe, destination_table_ref, chunksize, schema, location, api_method, write_disposition, billing_project)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_chunks\u001b[39m(\n\u001b[0;32m    240\u001b[0m     client,\n\u001b[0;32m    241\u001b[0m     dataframe,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    248\u001b[0m     billing_project: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    249\u001b[0m ):\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m api_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 251\u001b[0m         \u001b[43mload_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdestination_table_ref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwrite_disposition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m            \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbilling_project\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbilling_project\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;66;03m# TODO: yield progress depending on result() with timeout\u001b[39;00m\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\npleg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas_gbq\\load.py:139\u001b[0m, in \u001b[0;36mload_parquet\u001b[1;34m(client, dataframe, destination_table_ref, write_disposition, location, schema, billing_project)\u001b[0m\n\u001b[0;32m    136\u001b[0m     dataframe \u001b[38;5;241m=\u001b[39m cast_dataframe_for_parquet(dataframe, schema)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 139\u001b[0m     \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_table_from_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_table_ref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjob_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbilling_project\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mresult()\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pyarrow\u001b[38;5;241m.\u001b[39mlib\u001b[38;5;241m.\u001b[39mArrowInvalid \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mConversionError(\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not convert DataFrame to Parquet.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    149\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\npleg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\cloud\\bigquery\\client.py:2802\u001b[0m, in \u001b[0;36mClient.load_table_from_dataframe\u001b[1;34m(self, dataframe, destination, num_retries, job_id, job_id_prefix, location, project, job_config, parquet_compression, timeout)\u001b[0m\n\u001b[0;32m   2799\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parquet_compression \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msnappy\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# adjust the default value\u001b[39;00m\n\u001b[0;32m   2800\u001b[0m         parquet_compression \u001b[38;5;241m=\u001b[39m parquet_compression\u001b[38;5;241m.\u001b[39mupper()\n\u001b[1;32m-> 2802\u001b[0m     \u001b[43m_pandas_helpers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataframe_to_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2803\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2804\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnew_job_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2805\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtmppath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2806\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparquet_compression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparquet_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2807\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparquet_use_compliant_nested_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2808\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2809\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2810\u001b[0m     dataframe\u001b[38;5;241m.\u001b[39mto_parquet(\n\u001b[0;32m   2811\u001b[0m         tmppath,\n\u001b[0;32m   2812\u001b[0m         engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2818\u001b[0m         ),\n\u001b[0;32m   2819\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\npleg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\cloud\\bigquery\\_pandas_helpers.py:686\u001b[0m, in \u001b[0;36mdataframe_to_parquet\u001b[1;34m(dataframe, bq_schema, filepath, parquet_compression, parquet_use_compliant_nested_type)\u001b[0m\n\u001b[0;32m    679\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    680\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_compliant_nested_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: parquet_use_compliant_nested_type}\n\u001b[0;32m    681\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _versions_helpers\u001b[38;5;241m.\u001b[39mPYARROW_VERSIONS\u001b[38;5;241m.\u001b[39muse_compliant_nested_type\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m    683\u001b[0m )\n\u001b[0;32m    685\u001b[0m bq_schema \u001b[38;5;241m=\u001b[39m schema\u001b[38;5;241m.\u001b[39m_to_schema_fields(bq_schema)\n\u001b[1;32m--> 686\u001b[0m arrow_table \u001b[38;5;241m=\u001b[39m \u001b[43mdataframe_to_arrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbq_schema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    687\u001b[0m pyarrow\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mwrite_table(\n\u001b[0;32m    688\u001b[0m     arrow_table,\n\u001b[0;32m    689\u001b[0m     filepath,\n\u001b[0;32m    690\u001b[0m     compression\u001b[38;5;241m=\u001b[39mparquet_compression,\n\u001b[0;32m    691\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    692\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\npleg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\cloud\\bigquery\\_pandas_helpers.py:629\u001b[0m, in \u001b[0;36mdataframe_to_arrow\u001b[1;34m(dataframe, bq_schema)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bq_field \u001b[38;5;129;01min\u001b[39;00m bq_schema:\n\u001b[0;32m    627\u001b[0m     arrow_names\u001b[38;5;241m.\u001b[39mappend(bq_field\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m    628\u001b[0m     arrow_arrays\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 629\u001b[0m         \u001b[43mbq_to_arrow_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_column_or_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbq_field\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbq_field\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m     )\n\u001b[0;32m    631\u001b[0m     arrow_fields\u001b[38;5;241m.\u001b[39mappend(bq_to_arrow_field(bq_field, arrow_arrays[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtype))\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m((field \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m arrow_fields)):\n",
      "File \u001b[1;32mc:\\Users\\npleg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\cloud\\bigquery\\_pandas_helpers.py:344\u001b[0m, in \u001b[0;36mbq_to_arrow_array\u001b[1;34m(series, bq_field)\u001b[0m\n\u001b[0;32m    342\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mError converting Pandas column with name: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseries\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and datatype: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseries\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m to an appropriate pyarrow datatype: Array, ListArray, or StructArray\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    343\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39merror(msg)\n\u001b[1;32m--> 344\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m pyarrow\u001b[38;5;241m.\u001b[39mArrowTypeError(msg)\n",
      "\u001b[1;31mArrowTypeError\u001b[0m: Error converting Pandas column with name: \"organic\" and datatype: \"object\" to an appropriate pyarrow datatype: Array, ListArray, or StructArray"
     ]
    }
   ],
   "source": [
    "for file_name in clean_files:\n",
    "    df= pd.read_csv(data_directory + file_name)\n",
    "    table_name = file_name.replace(\"_processed.csv\", \"\")\n",
    "\n",
    "    full_table_name = f\"{gbq_dataset_ID}.{table_name}\"\n",
    "    to_gbq(df, full_table_name, project_id=gbq_project_ID, if_exists=\"replace\", credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "full_table_name = f\"{gbq_dataset_ID}.{table_name}\"\n",
    "to_gbq(df, full_table_name, project_id=gbq_project_ID, if_exists=\"replace\", credentials=credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop over to upload all Processed Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in os.listdir(data_directory):\n",
    "    if file_name.endswith(\"_small.csv_processed.csv\"):\n",
    "        # Construct full path and table name\n",
    "        full_path = os.path.join(data_directory, file_name)\n",
    "        table_name = file_name.replace(\"_small.csv_processed.csv\", \"\")  # Remove suffix\n",
    "        full_table_name = f\"{gbq_dataset_ID}.{table_name}\"  # Full table name\n",
    "\n",
    "        # Check if the file exists\n",
    "        if os.path.exists(full_path):\n",
    "            print(f\"Uploading {file_name} to BigQuery...\")\n",
    "\n",
    "            # Open the CSV and upload to BigQuery\n",
    "            with open(full_path, 'rb') as source_file:\n",
    "                job = client.load_table_from_file(\n",
    "                    source_file,\n",
    "                    full_table_name,\n",
    "                    job_config=job_config,\n",
    "                )\n",
    "                job.result()  # Wait for the job to complete\n",
    "                print(f\"{file_name} uploaded successfully to {full_table_name}.\")\n",
    "            break\n",
    "        else:\n",
    "            print(f\"File not found: {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_path= data_directory + current_zf\n",
    "if os.path.exists(full_path):\n",
    "    print(\"File found\")\n",
    "\n",
    "    with open(data_directory + current_zf, 'rb') as source_file:\n",
    "        job = client.load_table_from_file(\n",
    "            source_file,\n",
    "            f\"{gbq_dataset_id}.{current_zf}\",\n",
    "            job_config=job_config,\n",
    "        )\n",
    "else:\n",
    "    print(\"File not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_path= data_directory + current_zf\n",
    "if os.path.exists(full_path):\n",
    "    print(\"File found\")\n",
    "\n",
    "    with open(data_directory + current_zf, 'rb') as source_file:\n",
    "        job = client.load_table_from_file(\n",
    "            source_file,\n",
    "            f\"{gbq_dataset_id}.{current_zf}\",\n",
    "            job_config=job_config,\n",
    "        )\n",
    "else:\n",
    "    print(\"File not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for file in data_directory:\n",
    "    tab, _ = file.replace(\"_clean.csv\", \"\")\n",
    "    table_full_name = \".\".join([gbq_project_id, gbq_dataset_id, tab])\n",
    "\n",
    "    if not table_exists(client, table_full_name):\n",
    "        table_ref = client.create_table(table=table_full_name)\n",
    "    else:\n",
    "        table_ref = client.get_table(table_full_name)\n",
    "\n",
    "    table = client.get_table(table_ref)\n",
    "    print(\"Table {} contains {} columns\".format(table_ref.table_id, len(table.schema)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in clean_files:\n",
    "    df = pd.read_csv(data_path + file_name)\n",
    "    \n",
    "    # Replace NaN and empty strings with 'Unknown' in 'memType'\n",
    "    df['memType'] = df['memType'].fillna('Unknown').replace('', 'Unknown').astype(str)\n",
    "    \n",
    "    # Convert 'datetime' column to datetime type\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "    \n",
    "    # Fill NaN values in numeric columns only if they exist\n",
    "    numeric_columns = ['taxexempt', 'itemstatus', 'tenderstatus', 'batchHeaderID']\n",
    "    for col in numeric_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(0)\n",
    " \n",
    "    # Proceed with uploading to BigQuery\n",
    "    table_name = file_name.replace(\"_clean.csv\", \"\")\n",
    "    full_table_name = f'{dataset_id}.{table_name}'\n",
    "    \n",
    "    # Upload to GBQ\n",
    "    to_gbq(df, full_table_name, project_id=project_id, credentials=credentials, if_exists='replace')\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
