{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Task 1 Clean and Process the Zip Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import io\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "#from google.cloud import bigquery\n",
    "#from pandas_gbq import to_gbq\n",
    "#from google.oauth2 import service_account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "1. Extra quotes on some headers???  \n",
    "1. Loading with or without a header???\n",
    "1. I did not find any duplicates or didn't remove any null values\n",
    "1. When importing to GBQ I cannot get datetime to come in as TIMESTAMP vs string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Source Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"Data/LargeZips/\"\n",
    "zip_files = os.listdir(\"Data/LargeZips\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sniffing out the Delimiter \n",
    "This section identifies the delimiter for each csv file and stores it in a dictionary called **delimiters** with the file_name as the key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiters = dict() \n",
    "\n",
    "# Start by reading in all the files again.\n",
    "\n",
    "#current_zf = zip_files[0]\n",
    "for current_zf in zip_files :\n",
    "# Open the current zf\n",
    "    with ZipFile(data_directory + current_zf,'r') as zf :\n",
    "        zipped_files = zf.namelist()\n",
    "        \n",
    "        # Iteraate over each file inside the current zip file\n",
    "        for file_name in zipped_files :\n",
    "            # Open and wrap it to read as text\n",
    "            input_file = io.TextIOWrapper(zf.open(file_name, 'r'), encoding=\"utf-8\")\n",
    "            \n",
    "            dialect = csv.Sniffer().sniff(sample=input_file.readline(),\n",
    "                                      delimiters=[\",\",\";\",\"\\t\"])\n",
    "            \n",
    "            delimiters[file_name] = dialect.delimiter\n",
    "            \n",
    "            print(\" \".join([\"For\",\n",
    "                           file_name,\n",
    "                           \"the delimiter is\",\n",
    "                           dialect.delimiter\n",
    "                           ]))\n",
    "\n",
    "            input_file.close() # tidy up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Headers\n",
    "\n",
    "This sections iterates over each csv file to determine if the file contains a header row. The results are stored in a dictionary titled **Headers** with file_name as the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = dict()\n",
    "\n",
    "def is_header_row(first_row, second_row):\n",
    "\n",
    "    # Check if most elements in the first row contain non-numeric characters\n",
    "    if all(any(c.isalpha() for c in value) for value in first_row):\n",
    "        return True\n",
    "    \n",
    "    # Optionally: Check if types of first and second rows differ\n",
    "    if set(map(type, first_row)) != set(map(type, second_row)):\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "#current_zf = zip_files[0]\n",
    "for current_zf in zip_files :\n",
    "# Open the current zf\n",
    "    with ZipFile(data_directory + current_zf,'r') as zf :\n",
    "        zipped_files = zf.namelist()\n",
    "        \n",
    "        # Loop through each file in the zip\n",
    "        for file_name in zipped_files:\n",
    "            with zf.open(file_name, 'r') as input_file:\n",
    "                input_file = io.TextIOWrapper(input_file, encoding=\"utf-8\")\n",
    "\n",
    "                this_delimiter = delimiters.get(file_name, ',')  # Use delimiter or default to ','\n",
    "\n",
    "                # Read the first two lines\n",
    "                first_line = input_file.readline().strip().split(this_delimiter)\n",
    "                second_line = input_file.readline().strip().split(this_delimiter)\n",
    "\n",
    "                # Check for header presence using improved logic\n",
    "                has_header = is_header_row(first_line, second_line)\n",
    "                print(f\"File '{file_name}' has header: {has_header}\")\n",
    "\n",
    "                # Print first two lines for verification\n",
    "                print(\"First line:\", first_line)\n",
    "                print(\"Second line:\", second_line)\n",
    "\n",
    "\n",
    "                headers[file_name] = has_header\n",
    "\n",
    "\n",
    "                input_file.close()  # Close the file properly\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the dictionary\n",
    "if not headers:\n",
    "    print(\"The 'headers' dictionary is empty.\")\n",
    "else:\n",
    "    print(\"The 'headers' dictionary contains:\")\n",
    "    print(headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for current_zf in zip_files:\n",
    "    with ZipFile(data_directory + current_zf, 'r') as zf:\n",
    "        zipped_files = zf.namelist()\n",
    "\n",
    "        # Iterate over each file in the zip\n",
    "        for file_name in zipped_files:\n",
    "            # Assuming the file is a CSV, we will load it into a DataFrame\n",
    "            with zf.open(file_name) as file:\n",
    "                # Read file as CSV\n",
    "                try:\n",
    "                    df = pd.read_csv(file, quotechar='\"') \n",
    "                    print(f\"Loaded {file_name} into DataFrame.\")\n",
    "                   # print(df.head())  # Check the first few rows of the DataFrame\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {file_name}: {e}\")\n",
    "                    \n",
    "            initial_row_count = len(df)\n",
    "\n",
    "            # Remove duplicates\n",
    "            df_cleaned = df.drop_duplicates()\n",
    "\n",
    "            # Get the number of rows after removing duplicates\n",
    "            final_row_count = len(df_cleaned)\n",
    "\n",
    "            # Calculate the number of duplicates removed\n",
    "            duplicates_removed = initial_row_count - final_row_count\n",
    "\n",
    "            print(f\"Number of duplicates removed: {duplicates_removed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null Values\n",
    "\n",
    "trans_subtype       389327 string\n",
    "trans_status        673703 string\n",
    "percentDiscount    1094857 float\n",
    "memType            2998330 boolean\n",
    "charflag           2321518 string\n",
    "batchHeaderID      2998330 boolean\n",
    "organic            2998330 float\n",
    "display            2998330 boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a function to transform the date column in a dataframe to \n",
    "# the YYYYMM01 format we'd like to use for subsetting.\n",
    "\n",
    "def reformat_date(date_string) :\n",
    "    date_string = datetime.datetime.strptime(date_string,\"%Y-%m-%d\")\n",
    "    return(datetime.date.strftime(date_string,\"%Y%m\")+\"01\")\n",
    "\n",
    "assert(reformat_date(\"2022-09-20\")==\"20220901\")\n",
    "assert(reformat_date(\"2000-10-20\")==\"20001001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning and Processing\n",
    "-- same loop as above but with processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values(df, schema):\n",
    "    for field in schema:\n",
    "        field_name = field.name\n",
    "        field_type = field.field_type\n",
    "        \n",
    "        # Handle missing values based on field type\n",
    "        if field_type == \"FLOAT\":\n",
    "            df[field_name] = df[field_name].fillna(0.0)\n",
    "        elif field_type == \"STRING\":\n",
    "            df[field_name] = df[field_name].fillna('')\n",
    "        elif field_type == \"TIMESTAMP\":\n",
    "            df[field_name] = pd.to_datetime(df[field_name], errors='coerce')\n",
    "        elif field_type == \"BOOLEAN\":\n",
    "            df[field_name] = df[field_name].fillna(False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Files\n",
    "This sections iterates over all the zip files in the raw data directory. It then processes them and saves them in a new processed files directory as csv files. \n",
    "\n",
    "1. Check for delimiter\n",
    "1. Check for header\n",
    "1. Add header column if missing\n",
    "1. Handle Missing Values\n",
    "1. Correct datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_expected_types(df):\n",
    "    for col, dtype in expected_types.items():\n",
    "        if col in df.columns:\n",
    "            print(f\"Converting column '{col}' to {dtype}...\")\n",
    "            try:\n",
    "                if dtype == \"datetime64[ns]\":\n",
    "                    df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "                elif dtype == \"float64\":\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                elif dtype == \"Int64\":\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce').astype(\"Int64\")\n",
    "                elif dtype == \"boolean\":\n",
    "                    df[col] = df[col].astype(str).str.lower().map({\n",
    "                        'true': True, 'false': False, '1': True, '0': False, 'nan': pd.NA\n",
    "                    }).astype(\"boolean\")\n",
    "                elif dtype == \"object\":\n",
    "                    df[col] = df[col].astype(str).replace(\"nan\", pd.NA)\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting column '{col}': {e}\")\n",
    "                df[col] = df[col].astype(\"object\")\n",
    "                print(f\"Fallback: Converted '{col}' to object type.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "## Working Datatype column conversion and header/delimter#\n",
    "##########################################################\n",
    "processed_files_directory = \"data/processed_files/\"\n",
    "\n",
    "# Define common headers\n",
    "common_headers = [\n",
    "    'datetime', 'register_no', 'emp_no', 'trans_no', 'upc', 'description', \n",
    "    'trans_type', 'trans_subtype', 'trans_status', 'department', 'quantity', \n",
    "    'Scale', 'cost', 'unitPrice', 'total', 'regPrice', 'altPrice', 'tax', \n",
    "    'taxexempt', 'foodstamp', 'wicable', 'discount', 'memDiscount', \n",
    "    'discountable', 'discounttype', 'voided', 'percentDiscount', 'ItemQtty', \n",
    "    'volDiscType', 'volume', 'VolSpecial', 'mixMatch', 'matched', 'memType', \n",
    "    'staff', 'numflag', 'itemstatus', 'tenderstatus', 'charflag', 'varflag', \n",
    "    'batchHeaderID', 'local', 'organic', 'display', 'receipt', 'card_no', \n",
    "    'store', 'branch', 'match_id', 'trans_id'\n",
    "]\n",
    "\n",
    "# Define expected types\n",
    "expected_types = {\n",
    "    \"datetime\": \"datetime64[ns]\",\n",
    "    \"register_no\": \"float64\",\n",
    "    \"emp_no\": \"float64\",\n",
    "    \"trans_no\": \"float64\",\n",
    "    \"upc\": \"object\",\n",
    "    \"description\": \"object\",\n",
    "    \"trans_type\": \"object\",\n",
    "    \"trans_subtype\": \"object\",\n",
    "    \"trans_status\": \"object\",\n",
    "    \"department\": \"float64\",\n",
    "    \"quantity\": \"float64\",\n",
    "    \"Scale\": \"float64\",\n",
    "    \"cost\": \"float64\",\n",
    "    \"unitPrice\": \"float64\",\n",
    "    \"total\": \"float64\",\n",
    "    \"regPrice\": \"float64\",\n",
    "    \"altPrice\": \"float64\",\n",
    "    \"tax\": \"float64\",\n",
    "    \"taxexempt\": \"float64\",\n",
    "    \"foodstamp\": \"float64\",\n",
    "    \"wicable\": \"float64\",\n",
    "    \"discount\": \"float64\",\n",
    "    \"memDiscount\": \"float64\",\n",
    "    \"discountable\": \"float64\",\n",
    "    \"discounttype\": \"float64\",\n",
    "    \"voided\": \"float64\",\n",
    "    \"percentDiscount\": \"float64\",\n",
    "    \"ItemQtty\": \"float64\",\n",
    "    \"volDiscType\": \"object\",\n",
    "    \"volume\": \"float64\",\n",
    "    \"VolSpecial\": \"float64\",\n",
    "    \"mixMatch\": \"float64\",\n",
    "    \"matched\": \"float64\",\n",
    "    \"memType\": \"boolean\",\n",
    "    \"staff\": \"boolean\",\n",
    "    \"numflag\": \"float64\",\n",
    "    \"itemstatus\": \"float64\",\n",
    "    \"tenderstatus\": \"float64\",\n",
    "    \"charflag\": \"object\",\n",
    "    \"varflag\": \"object\",\n",
    "    \"batchHeaderID\": \"boolean\",\n",
    "    \"local\": \"float64\",\n",
    "    \"organic\": \"float64\",\n",
    "    \"display\": \"boolean\",\n",
    "    \"receipt\": \"float64\",\n",
    "    \"card_no\": \"float64\",\n",
    "    \"store\": \"float64\",\n",
    "    \"branch\": \"float64\",\n",
    "    \"match_id\": \"float64\",\n",
    "    \"trans_id\": \"float64\"\n",
    "}\n",
    "\n",
    "# Set the future option to silence the warning (optional)\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "def handle_null_values(df):\n",
    "    \"\"\"Replace known null-like values with NaN and ensure NaN is treated as None.\"\"\"\n",
    "    for col in df.columns:\n",
    "        print(f\"Handling null values for column: '{col}'...\")\n",
    "        try:\n",
    "            # Use raw strings to avoid unicode escape errors\n",
    "            df[col] = df[col].replace([r'nan', r'None', r'\\\\N', r'\\N'], np.nan)\n",
    "\n",
    "            # Ensure that the correct type is inferred\n",
    "            df[col] = df[col].infer_objects(copy=False)\n",
    "\n",
    "            # Ensure NaN becomes None for BigQuery compatibility\n",
    "            df[col] = df[col].where(~df[col].isna(), None)\n",
    "        except Exception as e:\n",
    "            print(f\"Error handling null values for column '{col}': {e}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function to sanitize and strictly convert to boolean\n",
    "def clean_boolean_column(df, col):\n",
    "    \"\"\"Strictly clean, validate, and convert a column to boolean.\"\"\"\n",
    "    print(f\"Cleaning boolean column '{col}'...\")\n",
    "\n",
    "    try:\n",
    "        # Normalize values to boolean-like or pd.NA\n",
    "        df[col] = df[col].astype(str).str.strip().str.lower().map({\n",
    "            'true': True, 'false': False, '1': True, '0': False, \n",
    "            '': pd.NA, 'nan': pd.NA, 'none': pd.NA\n",
    "        })\n",
    "\n",
    "        # Identify any remaining invalid values\n",
    "        invalid_values = df[col][~df[col].isin([True, False, pd.NA])]\n",
    "        if not invalid_values.empty:\n",
    "            print(f\"Warning: Invalid boolean values found in '{col}': {invalid_values.unique()}\")\n",
    "            # Replace invalid values with pd.NA\n",
    "            df[col] = df[col].where(df[col].isin([True, False]), pd.NA)\n",
    "\n",
    "        # Ensure the column has the correct dtype\n",
    "        df[col] = df[col].astype('boolean')\n",
    "        print(f\"Successfully converted '{col}' to boolean.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing column '{col}': {e}\")\n",
    "        # Log invalid entries for debugging\n",
    "        print(f\"Invalid entries: {df[col].dropna().unique()}\")\n",
    "        raise ValueError(f\"Failed to convert column '{col}' to boolean.\")\n",
    "\n",
    "\n",
    "# Function to assign expected data types\n",
    "\n",
    "                # Immediately raise the error to prevent bad data from continuing\n",
    "                #except Exception as e:\n",
    "                #print(f\"Error converting column '{col}': {e}. Falling back to 'object'.\")\n",
    "                #df[col] = df[col].astype(\"object\")   \n",
    "\n",
    "def assign_data_types(df):\n",
    "    \"\"\"Convert DataFrame columns to expected data types.\"\"\"\n",
    "    for col, dtype in expected_types.items():\n",
    "        if col in df.columns:\n",
    "            print(f\"Converting column '{col}' to {dtype}...\")\n",
    "            try:\n",
    "                if dtype == \"datetime64[ns]\":\n",
    "                    df[col] = pd.to_datetime(df[col], errors='raise', format=\"%Y-%m-%d %H:%M:%S\")\n",
    "                elif dtype == \"float64\":\n",
    "                    df[col] = pd.to_numeric(df[col], errors='raise')\n",
    "                elif dtype == \"Int64\":\n",
    "                    df[col] = pd.to_numeric(df[col], errors='raise').astype(\"Int64\")\n",
    "                elif dtype == \"boolean\":\n",
    "                    clean_boolean_column(df, col)  # Use new boolean conversion logic\n",
    "                elif dtype == \"object\":\n",
    "                    df[col] = df[col].astype(str).replace(\"nan\", pd.NA)\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting column '{col}': {e}\")\n",
    "                raise  # Stop the process if conversion fails\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_file(file_name, working_file):\n",
    "    delimiter = delimiters.get(file_name, ',')  # Default to ','\n",
    "    has_header = headers.get(file_name, True)  # Check if the file has a header\n",
    "    header = 0 if has_header else None  # 0 for header row, None if no header\n",
    "    \n",
    "\n",
    "    try:\n",
    "            # Load the CSV without parsing dates initially\n",
    "            df = pd.read_csv(\n",
    "                working_file,\n",
    "                delimiter=delimiter,\n",
    "                header=header,\n",
    "                dtype={col: dtype for col, dtype in expected_types.items() if col != \"datetime\"},\n",
    "                low_memory=False\n",
    "            )\n",
    "\n",
    "            # If the file doesn't have headers, assign them now\n",
    "            if not has_header:\n",
    "                df.columns = common_headers\n",
    "                print(f\"Assigned common headers to {file_name}.\")\n",
    "\n",
    "            # Now parse the datetime column, if it exists\n",
    "            if 'datetime' in df.columns:\n",
    "                print(f\"Parsing 'datetime' column for {file_name}...\")\n",
    "                df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "\n",
    "            print(f\"Loaded {file_name} successfully!\")\n",
    "            return df\n",
    "\n",
    "    except Exception as e:\n",
    "            print(f\"Error loading {file_name}: {e}\")\n",
    "            return None\n",
    "\n",
    "# Process ZIP files\n",
    "zip_files = os.listdir(data_directory)\n",
    "\n",
    "for current_zf in zip_files:\n",
    "    with ZipFile(os.path.join(data_directory, current_zf), 'r') as zf:\n",
    "        zipped_files = zf.namelist()\n",
    "\n",
    "        for file_name in zipped_files:\n",
    "            with zf.open(file_name) as working_file:\n",
    "                df = load_file(file_name, working_file)\n",
    "               \n",
    "                if df is not None:\n",
    "                    try:\n",
    "                        # Clean and process the DataFrame\n",
    "                        df.columns = df.columns.str.strip()\n",
    "                \n",
    "                        # Clean and prepare the DataFrame\n",
    "                        df = handle_null_values(df)\n",
    "                        df = assign_data_types(df)\n",
    "\n",
    "                        # Save the processed DataFrame\n",
    "                        processed_filename = os.path.join(\n",
    "                            processed_files_directory, f\"{os.path.splitext(file_name)[0]}_processed.csv\"\n",
    "                        )\n",
    "                        df.to_csv(processed_filename, index=False)\n",
    "                        print(f\"Processed and saved {file_name}.\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "    print(f\"Completed processing ZIP file: {current_zf}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a file with its metadata\n",
    "def load_file(file_name, working_file):\n",
    "    delimiter = delimiters.get(file_name, ',')  # Default to ','\n",
    "    has_header = headers.get(file_name, True)  # Check if the file has a header\n",
    "    header = 0 if has_header else None  # 0 for header row, None if no header\n",
    "\n",
    "    try:\n",
    "        # Load the CSV with the appropriate delimiter and header handling\n",
    "        df = pd.read_csv(working_file, delimiter=delimiter, header=header)\n",
    "\n",
    "        # Assign common headers if the file has no header\n",
    "        if not has_header:\n",
    "            df.columns = common_headers\n",
    "\n",
    "        print(f\"Loaded {file_name} successfully!\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Process ZIP files\n",
    "zip_files = os.listdir(data_directory)\n",
    "\n",
    "for current_zf in zip_files:\n",
    "    with ZipFile(os.path.join(data_directory, current_zf), 'r') as zf:\n",
    "        zipped_files = zf.namelist()\n",
    "\n",
    "        for file_name in zipped_files:\n",
    "            with zf.open(file_name) as working_file:\n",
    "                df = load_file(file_name, working_file)\n",
    "\n",
    "                if df is not None:\n",
    "                    try:\n",
    "                        # Clean and process the DataFrame\n",
    "                        df.columns = df.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process data one file at time to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_files_directory = \"data/processed_files/\"\n",
    "\n",
    "# Define common headers for files without headers\n",
    "common_headers = [\n",
    "    'datetime', 'register_no', 'emp_no', 'trans_no', 'upc', 'description', \n",
    "    'trans_type', 'trans_subtype', 'trans_status', 'department', 'quantity', \n",
    "    'Scale', 'cost', 'unitPrice', 'total', 'regPrice', 'altPrice', 'tax', \n",
    "    'taxexempt', 'foodstamp', 'wicable', 'discount', 'memDiscount', \n",
    "    'discountable', 'discounttype', 'voided', 'percentDiscount', 'ItemQtty', \n",
    "    'volDiscType', 'volume', 'VolSpecial', 'mixMatch', 'matched', 'memType', \n",
    "    'staff', 'numflag', 'itemstatus', 'tenderstatus', 'charflag', 'varflag', \n",
    "    'batchHeaderID', 'local', 'organic', 'display', 'receipt', 'card_no', \n",
    "    'store', 'branch', 'match_id', 'trans_id'\n",
    "]\n",
    "\n",
    "if not os.path.exists(processed_files_directory):\n",
    "    os.makedirs(processed_files_directory)\n",
    "\n",
    "# Function to load a file with its metadata\n",
    "def load_file(file_name, working_file):\n",
    "    delimiter = delimiters.get(file_name, ',')  # Default to ','\n",
    "    has_header = headers.get(file_name, True)  # Check if the file has a header\n",
    "    header = 0 if has_header else None  # 0 for header row, None if no header\n",
    "\n",
    "    try:\n",
    "        # Load the CSV with the appropriate delimiter and header handling\n",
    "        df = pd.read_csv(working_file, delimiter=delimiter, header=header)\n",
    "\n",
    "        # Assign common headers if the file has no header\n",
    "        if not has_header:\n",
    "            df.columns = common_headers\n",
    "\n",
    "        print(f\"Loaded {file_name} successfully!\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Process ZIP files\n",
    "zip_files = os.listdir(data_directory)\n",
    "\n",
    "for current_zf in zip_files:\n",
    "    with ZipFile(os.path.join(data_directory, current_zf), 'r') as zf:\n",
    "        zipped_files = zf.namelist()\n",
    "\n",
    "        for file_name in zipped_files:\n",
    "            with zf.open(file_name) as working_file:\n",
    "                df = load_file(file_name, working_file)\n",
    "\n",
    "                if df is not None:\n",
    "                    try:\n",
    "                        # Clean and process the DataFrame\n",
    "                        df.columns = df.columns.str.strip()\n",
    "\n",
    "                        # Safely handle 'register_no' and 'quantity' columns if they exist\n",
    "                        if 'register_no' in df.columns:\n",
    "                            df['register_no'] = df['register_no'].fillna(0)\n",
    "                        if 'quantity' in df.columns:\n",
    "                            df['quantity'] = df['quantity'].fillna(0)\n",
    "\n",
    "                        # Convert datetime if the column exists\n",
    "                        if 'datetime' in df.columns:\n",
    "                            df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "\n",
    "\n",
    "                        # Convert columns to appropriate data types\n",
    "                        float_columns = [\n",
    "                            'register_no', 'emp_no', 'trans_no', 'department',\n",
    "                            'Scale', 'tax', 'taxexempt', 'foodstamp', 'wicable',\n",
    "                            'discountable', 'discounttype', 'voided', 'local',\n",
    "                            'receipt', 'card_no', 'store', 'branch', 'match_id',\n",
    "                            'trans_id'\n",
    "                        ]\n",
    "                        df[float_columns] = df[float_columns].astype(float, errors='ignore')\n",
    "\n",
    "                        bool_columns = ['memType', 'staff', 'batchHeaderID', 'display']\n",
    "                        df[bool_columns] = df[bool_columns].astype(bool, errors='ignore')\n",
    "\n",
    "                        df['volDiscType'] = df.get('volDiscType', \"\").astype(str)\n",
    "\n",
    "                        # Extract the base name without extensions before adding _processed.csv\n",
    "                        file_name_no_ext = os.path.splitext(file_name)[0]\n",
    "\n",
    "                        # Save the processed file\n",
    "                        processed_filename = os.path.join(\n",
    "                            processed_files_directory, f\"{file_name_no_ext}_processed.csv\"\n",
    "                        )\n",
    "                        df.to_csv(processed_filename, index=False)\n",
    "\n",
    "                        print(f\"Processed and saved {file_name}.\")\n",
    "                    except KeyError as e:\n",
    "                        print(f\"Missing expected column: {e}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "        print(f\"Completed processing ZIP file: {current_zf}\\n\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DOUBLE CHECK CSV headers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all files in the directory\n",
    "csv_files = [f for f in os.listdir(processed_files_directory) if f.endswith('.csv')]\n",
    "\n",
    "# Function to print headers of each CSV\n",
    "def print_csv_headers(file_name, file_path):\n",
    "    try:\n",
    "        # Read only the first 5 rows to print headers\n",
    "        df = pd.read_csv(file_path, nrows=5)\n",
    "        print(f\"Headers for {file_name}: {list(df.columns)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_name}: {e}\")\n",
    "\n",
    "# Loop through and print headers for each CSV file\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(processed_files_directory, csv_file)\n",
    "    print_csv_headers(csv_file, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate Over Zip Files and Save to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over all zip files in the directory\n",
    "for current_zf in zip_files:\n",
    "    with ZipFile(data_directory + current_zf, 'r') as zf:\n",
    "        zipped_files = zf.namelist()\n",
    "\n",
    "        # Iterate over each file in the zip\n",
    "        for file_name in zipped_files:\n",
    "            # Assuming the file is a CSV, we will load it into a DataFrame\n",
    "            with zf.open(file_name) as file:\n",
    "                # Read file as CSV\n",
    "                try:\n",
    "                    df = pd.read_csv(file, quotechar='\"') \n",
    "                    print(f\"Loaded {file_name} into DataFrame.\")\n",
    "                    print(df.head())  # Check the first few rows of the DataFrame\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {file_name}: {e}\")\n",
    "                # Ensure datetime column is properly formatted\n",
    "                df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "\n",
    "                # Convert integer columns to float where needed\n",
    "                float_columns = [\n",
    "                    'register_no', 'emp_no', 'trans_no', 'department', 'Scale', 'tax', \n",
    "                    'taxexempt', 'foodstamp', 'wicable', 'discountable', 'discounttype', \n",
    "                    'voided', 'local', 'receipt', 'card_no', 'store', 'branch', 'match_id', \n",
    "                    'trans_id'\n",
    "                ]\n",
    "                df[float_columns] = df[float_columns].astype(float)\n",
    "\n",
    "                # Convert specific columns to boolean\n",
    "                bool_columns = ['memType', 'staff', 'batchHeaderID', 'display']\n",
    "                df[bool_columns] = df[bool_columns].astype(bool)\n",
    "\n",
    "                # Convert columns to string if needed\n",
    "                df['volDiscType'] = df['volDiscType'].astype(str)\n",
    "\n",
    "                # Verify the new schema\n",
    "                print(df.dtypes)\n",
    "                    \n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns.tolist()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over all zip files in the directory\n",
    "for current_zf in zip_files:\n",
    "    with ZipFile(data_directory + current_zf, 'r') as zf:\n",
    "        zipped_files = zf.namelist()\n",
    "\n",
    "        # Iterate over each file in the zip\n",
    "        for file_name in zipped_files:\n",
    "            # Assuming the file is a CSV, we will load it into a DataFrame\n",
    "            with zf.open(file_name) as file:\n",
    "                try:\n",
    "                    # Read file as CSV\n",
    "                    df = pd.read_csv(file, quotechar='\"')\n",
    "                    print(f\"Loaded {file_name} into DataFrame.\")\n",
    "                    print(df.head())  # Check the first few rows\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {file_name}: {e}\")\n",
    "                    continue  # Skip to the next file on error\n",
    "\n",
    "                # Handle 'datetime' column if it exists\n",
    "                if 'datetime' in df.columns:\n",
    "                    df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "                else:\n",
    "                    print(f\"Warning: 'datetime' column not found in {file_name}.\")\n",
    "\n",
    "                # Convert integer columns to float where needed (only if they exist)\n",
    "                float_columns = [\n",
    "                    'register_no', 'emp_no', 'trans_no', 'department', 'Scale', 'tax', \n",
    "                    'taxexempt', 'foodstamp', 'wicable', 'discountable', 'discounttype', \n",
    "                    'voided', 'local', 'receipt', 'card_no', 'store', 'branch', 'match_id', \n",
    "                    'trans_id'\n",
    "                ]\n",
    "                for col in float_columns:\n",
    "                    if col in df.columns:\n",
    "                        df[col] = df[col].astype(float, errors='ignore')\n",
    "\n",
    "                # Convert specific columns to boolean (only if they exist)\n",
    "                bool_columns = ['memType', 'staff', 'batchHeaderID', 'display']\n",
    "                for col in bool_columns:\n",
    "                    if col in df.columns:\n",
    "                        df[col] = df[col].astype(bool, errors='ignore')\n",
    "\n",
    "                # Convert 'volDiscType' to string if it exists\n",
    "                if 'volDiscType' in df.columns:\n",
    "                    df['volDiscType'] = df['volDiscType'].astype(str)\n",
    "\n",
    "                # Verify the new schema\n",
    "                print(\"Updated DataFrame types:\")\n",
    "                print(df.dtypes)\n",
    "\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print out first line test (to be removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for this_zf in zip_files :\n",
    "    with ZipFile(data_directory + this_zf,'r') as zf :\n",
    "        zipped_files = zf.namelist()\n",
    "\n",
    "        for file_name in zipped_files :\n",
    "            input_file = zf.open(file_name,'r')\n",
    "            input_file = io.TextIOWrapper(input_file,encoding=\"utf-8\")\n",
    "            \n",
    "            this_delimiter = delimiters[file_name]\n",
    "            \n",
    "            #for line in input_file :\n",
    "                #print(line.strip().split(this_delimiter))\n",
    "                #break\n",
    "\n",
    "\n",
    "            for line in input_file:\n",
    "                #Split the line using the delimiter and remove quotes\n",
    "                cleaned_line = [item.replace('\"', '').strip() for item in line.strip().split(this_delimiter)]\n",
    "                \n",
    "                # Print the cleaned line\n",
    "                print(cleaned_line)\n",
    "                break      \n",
    "            input_file.close() # tidy up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to Align to Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure datetime column is properly formatted\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "\n",
    "# Convert integer columns to float where needed\n",
    "float_columns = [\n",
    "    'register_no', 'emp_no', 'trans_no', 'department', 'Scale', 'tax', \n",
    "    'taxexempt', 'foodstamp', 'wicable', 'discountable', 'discounttype', \n",
    "    'voided', 'local', 'receipt', 'card_no', 'store', 'branch', 'match_id', \n",
    "    'trans_id'\n",
    "]\n",
    "df[float_columns] = df[float_columns].astype(float)\n",
    "\n",
    "# Convert specific columns to boolean\n",
    "bool_columns = ['memType', 'staff', 'batchHeaderID', 'display']\n",
    "df[bool_columns] = df[bool_columns].astype(bool)\n",
    "\n",
    "# Convert columns to string if needed\n",
    "df['volDiscType'] = df['volDiscType'].astype(str)\n",
    "\n",
    "# Verify the new schema\n",
    "print(df.dtypes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
